{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio de la mejor red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras as ker\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_RAW = pd.read_csv('../Data/iris.data', sep=',')\n",
    "\n",
    "iris_entrenamiento, iris_test = train_test_split(iris_RAW, test_size = 0.20, shuffle=True) # El entreno es el 80% y el conjunto prueba 20%\n",
    "\n",
    "# Generamos las salidas de las diferentes listas\n",
    "esperadas_entrenamiento = iris_entrenamiento.pop(\"class\")\n",
    "esperadas_test = iris_test.pop(\"class\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación entradas y salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización entradas en base a máximo y mínimo (máximo es 1 - mínimo es 0)\n",
    "\n",
    "irisE = (iris_entrenamiento - iris_entrenamiento.min()) / (iris_entrenamiento.max() - iris_entrenamiento.min())\n",
    "irisT = (iris_test - iris_test.min()) / (iris_test.max() - iris_test.min())\n",
    "\n",
    "entradasE =  irisE.to_numpy() # Las 30 posibles variables\n",
    "esperadasE = esperadas_entrenamiento.to_numpy() # Array de B/M con el diagnóstico\n",
    "\n",
    "entradasT = irisT.to_numpy() \n",
    "esperadasT = esperadas_test.to_numpy()\n",
    "\n",
    "# Categorizamos el diagnostico de entreno \n",
    "label_encoder = LabelEncoder()\n",
    "esperadasE = label_encoder.fit_transform(esperadasE)\n",
    "\n",
    "# Categorizamos el diagnostico de predicción \n",
    "esperadasT = label_encoder.fit_transform(esperadasT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables a tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nNeuronasCapaOculta = 12 # Tocar\n",
    "Epoch = 500 # Tocar\n",
    "tam_entrada = irisE.shape[1] # Tamaño de la capa de entrada = columnas\n",
    "\n",
    "automatizador = True # Ejecutar el automatizador para encontrar los parametros que mas se ajustan a los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación y ajuste de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initRed(nNeuronasCapaOculta, tam_entrada):\n",
    "    MLP = ker.Sequential() # Creamos el modelo vacío\n",
    "\n",
    "    # Añadimos la primera capa (tecnicamente dos, la de entrada (con la variable input_dim) y la oculta)\n",
    "    MLP.add(ker.layers.Dense(nNeuronasCapaOculta, input_dim = tam_entrada, activation='relu'))\n",
    "\n",
    "    # Añadimos la capa de salida\n",
    "    MLP.add(ker.layers.Dense(1,activation='sigmoid')) \n",
    "\n",
    "    # MLP.summary()\n",
    "\n",
    "    # Por último tenemos que configurar el modelo antes de entrenarlo con lo dicho en el enunciado\n",
    "    # Optimizador: Adam\n",
    "    # Función error: Mean Squared Error\n",
    "    # Metricas = Binary accuracy porque nos da el número de precisión de la red\n",
    "    MLP.compile(optimizer='adam',loss='mean_squared_error',metrics=[\"binary_accuracy\"])\n",
    "    return MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(MLP,Epoch,entradas,esperadas):#,entradasVal,esperadasVal):\n",
    "    # Por último entrenamos la red con el epoch (nº iteraciones)\n",
    "    output_fit = MLP.fit(entradas,esperadas,validation_split=0.20,epochs=Epoch,verbose = 0) #validation_data=() % 20% de los datos se usan para validar\n",
    "    return output_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calidad de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pintarExact(output_fit):\n",
    "    print(output_fit.history.keys())\n",
    "    # Pintamos las tanto la loss como la accuracy\n",
    "    plt.plot(output_fit.history['loss'])\n",
    "    plt.plot(output_fit.history['binary_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Error', 'Exactitud'], loc='lower left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar(MLP,entradas,esperadas):\n",
    "    # Evaluamos el modelo\n",
    "    print(\"Datos evaluados\")\n",
    "    evaluar = MLP.evaluate(entradas, esperadas,verbose=0)\n",
    "    print(\"Error, Exactitud:\", evaluar)\n",
    "    return evaluar\n",
    "\n",
    "def evaluarAuto(MLP,entradas,esperadas):\n",
    "    return MLP.evaluate(entradas, esperadas,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de nuestra mejor red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not automatizador:\n",
    "    MLP = initRed(nNeuronasCapaOculta, tam_entrada)\n",
    "    output_fit = entrenar(MLP,Epoch,entradasE,esperadasE)#,entradasV,esperadasV) # conjunto de entrenamiento junto con el conjunto de validación\n",
    "    pintarExact(output_fit)\n",
    "    evaluacion = evaluar(MLP,entradasT,esperadasT)  # conjunto de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudio del Nº Neuronas óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuronas\n",
    "errorF = []\n",
    "accuracyF = []\n",
    "if automatizador:\n",
    "    neuronas = [i for i in range(5,40,5)]\n",
    "    epochs = [i for i in range(50,1000,25)]\n",
    "    precision = 5\n",
    "    # Una vez hemos inicializado, podemos empezar a iterar con las listas creadas para encontrar la mejor configuración para los datos \n",
    "    err = []\n",
    "    ex = []\n",
    "    outputErr = [err,ex]\n",
    "    for neurona in neuronas:\n",
    "        print(\"Numero de neuronas:\",neurona,end=\"\\r\")\n",
    "        for i in range(precision):\n",
    "            MLP = initRed(neurona, tam_entrada)\n",
    "            output_fit = entrenar(MLP,epochs[int(len(epochs)/2)],entradasE,esperadasE)\n",
    "            error,exact = evaluarAuto(MLP,entradasT,esperadasT)\n",
    "            err.append(error)\n",
    "            ex.append(exact)\n",
    "        outputErr[0].append(np.mean(err))\n",
    "        outputErr[1].append(np.mean(exact))\n",
    "    errorF = outputErr[0].copy()\n",
    "    accuracyF = outputErr[1].copy()\n",
    "    # Ahora pintamos la grafica para ver el error y la precisión\n",
    "    plt.title('Precision')\n",
    "    plt.plot(outputErr[0])\n",
    "    plt.plot(outputErr[1])\n",
    "    plt.legend(['Error', 'Exactitud'], loc='center right')\n",
    "    plt.ylabel('Error & Exactitud')\n",
    "    plt.xlabel('Numero de neuronas por capa')\n",
    "    plt.show()\n",
    "    print(\"Iter , loss, accuracy\")\n",
    "    for i in range(len(errorF)):\n",
    "        print(i, errorF[i],accuracyF[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "\n",
    "- LAB02 de nuestro grupo "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae011bd53887c3dab0246b667a51a9d69f3606ce064e4af6110c47693a17202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow-environment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
